{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexisross/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/alexisross/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import nlp_train_utils\n",
    "importlib.reload(nlp_train_utils)\n",
    "from nlp_train_utils import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexisross/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "def load_model(device, model_name = 'bert-base-uncased'):\n",
    "    print(\"loading model...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(\"done.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_sst_data(file_path):\n",
    "    with open(file_path, \"r\") as data_file:\n",
    "        lines = list(data_file.readlines())\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip(\"\\n\")\n",
    "            if not line:\n",
    "                continue\n",
    "            parsed_line = Tree.fromstring(line)\n",
    "            text = (TreebankWordDetokenizer().detokenize(parsed_line.leaves()))\n",
    "            sentiment = int(parsed_line.label())\n",
    "            if sentiment < 2:\n",
    "                sentiment = 0.0\n",
    "            elif sentiment == 2:\n",
    "                continue\n",
    "            else:\n",
    "                sentiment = 1.0\n",
    "            texts.append(text)\n",
    "            labels.append(sentiment)\n",
    "    return texts, labels\n",
    "\n",
    "def antonyms(term):\n",
    "    response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    return [span.text for span in soup.findAll('a', {'class': 'css-4elvh4'})] # class = .css-7854fb for less relevant\n",
    "\n",
    "\n",
    "def get_candidates(model, text, max_candidates = 1):\n",
    "    words = word_tokenize(text)\n",
    "    candidates = [None] * max_candidates\n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        if wn.synsets(word) == []:\n",
    "            continue\n",
    "        tmp = wn.synsets(word)[0].pos()\n",
    "        # if not adjective or noun, continue\n",
    "        if tmp != \"a\" and tmp != \"n\":\n",
    "            continue\n",
    "        for a in antonyms(word):\n",
    "            candidates[counter] = (TreebankWordDetokenizer().detokenize([a if x == word else x for x in words]))\n",
    "            counter += 1\n",
    "            if counter >= max_candidates:\n",
    "                return list(filter(None.__ne__, candidates))\n",
    "    return list(filter(None.__ne__, candidates))\n",
    "\n",
    "def get_delta_opt(model, tokenizer, device, text):\n",
    "    cands = get_candidates(model, text)\n",
    "    max_prob = 0\n",
    "    found_cand = False\n",
    "    for c in cands:\n",
    "        cand_logits, cand_labels, cand_prob = get_pred(model, tokenizer, device, c, 1.0)\n",
    "        print(cand_prob)\n",
    "        if cand_prob > max_prob:\n",
    "            max_cand = c\n",
    "            max_prob = cand_prob\n",
    "            max_logits = cand_logits\n",
    "            max_prob = cand_prob\n",
    "            found_cand = True\n",
    "        else:\n",
    "            del cand_logits\n",
    "            del cand_labels\n",
    "            del cand_prob\n",
    "            torch.cuda.empty_cache()\n",
    "    if not found_cand:\n",
    "        max_cand, max_logits, max_prob = get_pred(model, tokenizer, device, text, 1.0)\n",
    "    return max_cand, max_logits, max_prob\n",
    "\n",
    "def get_pred(model, tokenizer, device, text, label):\n",
    "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "    input_ids = encoding.to(device)\n",
    "    labels = torch.LongTensor([label]).to(device)\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs[0]\n",
    "    pos_prob = torch.nn.Softmax(dim=-1)(logits)[:, -1]\n",
    "    return logits, labels, pos_prob\n",
    "\n",
    "\n",
    "def train_nlp(model, tokenizer, weight_dir, thresholds_to_eval, recourse_loss_weight):\n",
    "\n",
    "    # get data\n",
    "    device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # load model and tokenizer    \n",
    "    train_texts, train_labels = get_sst_data('data/nlp_data/train.txt')\n",
    "    dev_texts, dev_labels = get_sst_data('data/nlp_data/dev.txt')\n",
    "\n",
    "    batch_size = 32\n",
    "    threshold = 0.5\n",
    "\n",
    "    lr = 2e-5\n",
    "    num_warmup_steps = 0\n",
    "    num_epochs = 3\n",
    "    num_train_steps = len(train_texts)/batch_size * num_epochs\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps, num_train_steps)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight):\n",
    "        normal_loss = loss_fn(logits, labels)\n",
    "        print(\"delta logits: \", delta_logits)\n",
    "        recourse_loss = loss_fn(delta_logits, torch.LongTensor([1.0]).to(device))\n",
    "        return recourse_loss * recourse_loss_weight + normal_loss\n",
    "\n",
    "    best_val_loss = 100000000\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss, train_epoch_loss, train_correct = 0, 0, 0\n",
    "        \n",
    "        print(\"EPOCH: \", epoch)\n",
    "        model.train()\n",
    "\n",
    "        for i, (text, label) in tqdm(enumerate(zip(train_texts, train_labels)), total = len(train_texts)):\n",
    "            logits, labels, pos_prob = get_pred(model, tokenizer, device, text, label)\n",
    "            _, delta_logits, _ = get_delta_opt(model, tokenizer, device, text)\n",
    "            batch_loss += combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight)\n",
    "                        \n",
    "            if i % 1000 == 0:\n",
    "                print(i, \" out of \", len(train_texts))\n",
    "            \n",
    "            if i % batch_size == 0:    \n",
    "                model.zero_grad() \n",
    "                batch_loss.backward()\n",
    "                train_epoch_loss += batch_loss.item()\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "                del batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_loss = 0\n",
    "                \n",
    "        print(\"Train acc: \", train_correct/len(train_texts))\n",
    "        print(\"Train epoch loss: \", train_epoch_loss/len(train_texts))\n",
    "        \n",
    "        model.eval()\n",
    "            \n",
    "        val_correct = 0\n",
    "        epoch_val_loss = 0\n",
    "        pos_probs = []\n",
    "\n",
    "        flipped_by_thresh = {thresh: 0 for thresh in thresholds_to_eval}\n",
    "        negative_by_thresh = {thresh :0 for thresh in thresholds_to_eval}\n",
    "\n",
    "        for i, (text, label) in tqdm(enumerate(zip(dev_texts, dev_labels)), total = len(dev_texts)):\n",
    "            logits, labels, pos_prob = get_pred(model, tokenizer, device, text, label)\n",
    "            _, delta_logits, delta_prob = get_delta_opt(model, tokenizer, device, text)\n",
    "            epoch_val_loss += combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight)\n",
    "\n",
    "\n",
    "            del input_ids\n",
    "            \n",
    "            pos_probs.append(pos_prob.item())\n",
    "\n",
    "            for t in thresholds_to_eval:\n",
    "                if pos_prob.item() < t:\n",
    "                    negative_by_thresh[t] += 1\n",
    "                    if delta_prob >= t:\n",
    "                        flipped_by_thresh[t] += 1\n",
    "            \n",
    "        if epoch_val_los < best_val_loss:\n",
    "            best_model_name = weight_dir + str(recourse_loss_weight) + 'best_model.pt'\n",
    "            torch.save(model, best_model_name)\n",
    "            best_epoch = True\n",
    "\n",
    "        else:\n",
    "            best_epoch = False\n",
    "\n",
    "        # if best epoch, eval\n",
    "        if best_epoch:\n",
    "            np_probs = np.array(pos_probs)\n",
    "            np_labels = np.array(dev_labels)\n",
    "\n",
    "            f1_by_thresh, recall_by_thresh, precision_by_thresh, acc_by_thresh, flipped_proportion_by_thresh, recourse_proportion_by_thresh = [], [], [], [], [], []\n",
    "\n",
    "            for t_idx, t in enumerate(thresholds_to_eval):\n",
    "                label_preds = np.array([0.0 if a < t else 1.0 for a in np_probs])\n",
    "\n",
    "                f1 = round(f1_score(label_preds, np_labels), 3)\n",
    "                f1_by_thresh.append(f1) \n",
    "\n",
    "                recall = round(recall_score(label_preds, np_labels), 3)\n",
    "                recall_by_thresh.append(recall)\n",
    "\n",
    "                prec = round(precision_score(label_preds, np_labels), 3)\n",
    "                precision_by_thresh.append(prec)\n",
    "\n",
    "                acc = round(np.sum(label_preds == np_labels)/np_labels.shape[0], 3)\n",
    "                acc_by_thresh.append(acc) \n",
    "\n",
    "                num_neg = negative_by_thresh[t]\n",
    "                num_pos = len(dev_labels) - num_neg\n",
    "                assert (num_neg + num_pos) == len(dev_labels)\n",
    "                flipped = flipped_by_thresh[t]\n",
    "\n",
    "                if num_neg != 0:\n",
    "                    flipped_proportion = round(flipped/num_neg, 3)\n",
    "                else:\n",
    "                    flipped_proportion = 0\n",
    "\n",
    "                recourse_proportion = round((flipped + num_pos)/len(dev_labels), 3)\n",
    "\n",
    "                flipped_proportion_by_thresh.append(flipped_proportion)\n",
    "                recourse_proportion_by_thresh.append(recourse_proportion)\n",
    "\n",
    "            \n",
    "            thresholds_data = {}\n",
    "\n",
    "            thresholds_data['thresholds'] = thresholds_to_eval\n",
    "            thresholds_data['precisions'] = precision_by_thresh\n",
    "            thresholds_data['flipped_proportion'] = flipped_proportion_by_thresh\n",
    "            thresholds_data['recourse_proportion'] = recourse_proportion_by_thresh\n",
    "            thresholds_data['f1s'] = f1_by_thresh\n",
    "            thresholds_data['accs'] = acc_by_thresh\n",
    "            thresholds_data['recalls'] = recall_by_thresh\n",
    "            thresholds_data['precisions'] = precision_by_thresh\n",
    "\n",
    "            thresholds_df = pd.DataFrame(data=thresholds_data)\n",
    "            best_model_thresholds_file_name = weight_dir + str(recourse_loss_weight) + '_val_thresholds_info.csv'\n",
    "            thresholds_df.to_csv(best_model_thresholds_file_name, index_label='index')\n",
    "            \n",
    "        print(\"VAL ACC: \", val_correct/len(dev_texts))\n",
    "        print(\"+ \", val_preds.count(1.0))\n",
    "        print(\"-\", val_preds.count(0.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "recourse_loss_weight = 0.0\n",
    "weight_dir = 'test_nlp/' + str(recourse_loss_weight)\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "    \n",
    "thresholds_to_eval = [0.5, 0.6]\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model, tokenizer = load_model(device, model_name = 'bert-base-uncased')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "119c303da94e460fa77563c56018746b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=6920.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3690], device='cuda:1', grad_fn=<SelectBackward>)\n",
      "delta logits:  tensor([[ 0.3082, -0.2282]], device='cuda:1', grad_fn=<AddmmBackward>)\n",
      "0  out of  6920\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 10.76 GiB total capacity; 774.00 MiB already allocated; 6.56 MiB free; 864.00 MiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f4cd777b1e2 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7f4cd79d164b in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7f4cd79d2464 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7f4cd79d2aa1 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f4c87d3890e in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7f4c86172949 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7f4c8618c777 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7f4cc0f28c7d in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7f4cc0f28f97 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f4cc1033a1a in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::mm_cuda(at::Tensor const&, at::Tensor const&) + 0x6c (0x7f4c87227ffc in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf22a20 (0x7f4c86161a20 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xa56530 (0x7f4cc0895530 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f4cc107d81c in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::mm(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f4cc0fce6ab in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ed0a2f (0x7f4cc2d0fa2f in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0xa56530 (0x7f4cc0895530 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f4cc107d81c in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::Tensor::mm(at::Tensor const&) const + 0x4b (0x7f4cc1163cab in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2d11fbb (0x7f4cc2b50fbb in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x25f (0x7f4cc2b6c7df in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x3375bb7 (0x7f4cc31b4bb7 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f4cc31b0400 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f4cc31b0fa1 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f4cc31a9119 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f4ce853f4ba in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #26: <unknown function> + 0xbd6df (0x7f4cf41d46df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #27: <unknown function> + 0x76db (0x7f4cf9bf46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #28: clone + 0x3f (0x7f4cf9f2da3f in /lib/x86_64-linux-gnu/libc.so.6)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e37483c0b69c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_nlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mthresholds_to_eval\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecourse_loss_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-535e9cb2a75d>\u001b[0m in \u001b[0;36mtrain_nlp\u001b[0;34m(model, tokenizer, weight_dir, thresholds_to_eval, recourse_loss_weight)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m                 \u001b[0mtrain_epoch_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mbatch_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m                 \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         \"\"\"\n\u001b[0;32m--> 185\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m    125\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    126\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 1; 10.76 GiB total capacity; 774.00 MiB already allocated; 6.56 MiB free; 864.00 MiB reserved in total by PyTorch)\nException raised from malloc at /pytorch/c10/cuda/CUDACachingAllocator.cpp:272 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7f4cd777b1e2 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10.so)\nframe #1: <unknown function> + 0x1e64b (0x7f4cd79d164b in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #2: <unknown function> + 0x1f464 (0x7f4cd79d2464 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #3: <unknown function> + 0x1faa1 (0x7f4cd79d2aa1 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libc10_cuda.so)\nframe #4: at::native::empty_cuda(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0x11e (0x7f4c87d3890e in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #5: <unknown function> + 0xf33949 (0x7f4c86172949 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #6: <unknown function> + 0xf4d777 (0x7f4c8618c777 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #7: <unknown function> + 0x10e9c7d (0x7f4cc0f28c7d in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #8: <unknown function> + 0x10e9f97 (0x7f4cc0f28f97 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #9: at::empty(c10::ArrayRef<long>, c10::TensorOptions const&, c10::optional<c10::MemoryFormat>) + 0xfa (0x7f4cc1033a1a in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #10: at::native::mm_cuda(at::Tensor const&, at::Tensor const&) + 0x6c (0x7f4c87227ffc in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #11: <unknown function> + 0xf22a20 (0x7f4c86161a20 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so)\nframe #12: <unknown function> + 0xa56530 (0x7f4cc0895530 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #13: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f4cc107d81c in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #14: at::mm(at::Tensor const&, at::Tensor const&) + 0x4b (0x7f4cc0fce6ab in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #15: <unknown function> + 0x2ed0a2f (0x7f4cc2d0fa2f in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #16: <unknown function> + 0xa56530 (0x7f4cc0895530 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #17: at::Tensor c10::Dispatcher::call<at::Tensor, at::Tensor const&, at::Tensor const&>(c10::TypedOperatorHandle<at::Tensor (at::Tensor const&, at::Tensor const&)> const&, at::Tensor const&, at::Tensor const&) const + 0xbc (0x7f4cc107d81c in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #18: at::Tensor::mm(at::Tensor const&) const + 0x4b (0x7f4cc1163cab in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #19: <unknown function> + 0x2d11fbb (0x7f4cc2b50fbb in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #20: torch::autograd::generated::MmBackward::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&&) + 0x25f (0x7f4cc2b6c7df in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #21: <unknown function> + 0x3375bb7 (0x7f4cc31b4bb7 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #22: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&, torch::autograd::Node*, torch::autograd::InputBuffer&, std::shared_ptr<torch::autograd::ReadyQueue> const&) + 0x1400 (0x7f4cc31b0400 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #23: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&) + 0x451 (0x7f4cc31b0fa1 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #24: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x89 (0x7f4cc31a9119 in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_cpu.so)\nframe #25: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&, bool) + 0x4a (0x7f4ce853f4ba in /home/alexisross/.local/lib/python3.6/site-packages/torch/lib/libtorch_python.so)\nframe #26: <unknown function> + 0xbd6df (0x7f4cf41d46df in /usr/lib/x86_64-linux-gnu/libstdc++.so.6)\nframe #27: <unknown function> + 0x76db (0x7f4cf9bf46db in /lib/x86_64-linux-gnu/libpthread.so.0)\nframe #28: clone + 0x3f (0x7f4cf9f2da3f in /lib/x86_64-linux-gnu/libc.so.6)\n"
     ]
    }
   ],
   "source": [
    "train_nlp(model, tokenizer, weight_dir, thresholds_to_eval, recourse_loss_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
