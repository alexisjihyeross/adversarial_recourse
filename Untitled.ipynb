{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexisross/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "\n",
    "from nltk.tree import Tree\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device, model_name = 'bert-base-uncased'):\n",
    "    print(\"loading model...\")\n",
    "    model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "    print(\"done.\")\n",
    "    return model, tokenizer\n",
    "\n",
    "def get_sst_data(file_path):\n",
    "    with open(file_path, \"r\") as data_file:\n",
    "        lines = list(data_file.readlines())\n",
    "        texts = []\n",
    "        labels = []\n",
    "        for i, line in enumerate(lines):\n",
    "            line = line.strip(\"\\n\")\n",
    "            if not line:\n",
    "                continue\n",
    "            parsed_line = Tree.fromstring(line)\n",
    "            text = (TreebankWordDetokenizer().detokenize(parsed_line.leaves()))\n",
    "            sentiment = int(parsed_line.label())\n",
    "            if sentiment < 2:\n",
    "                sentiment = 0.0\n",
    "            elif sentiment == 2:\n",
    "                continue\n",
    "            else:\n",
    "                sentiment = 1.0\n",
    "            texts.append(text)\n",
    "            labels.append(sentiment)\n",
    "    return texts, labels\n",
    "\n",
    "def antonyms(term):\n",
    "    max_tries = 100\n",
    "    counter = 0\n",
    "    while counter < max_tries:\n",
    "        try:\n",
    "            response = requests.get('https://www.thesaurus.com/browse/{}'.format(term))\n",
    "            soup = BeautifulSoup(response.text, 'lxml')\n",
    "            return [span.text for span in soup.findAll('a', {'class': 'css-4elvh4'})] # class = .css-7854fb for less relevant\n",
    "        except:\n",
    "            counter += 1\n",
    "            continue\n",
    "    return None\n",
    "\n",
    "def get_candidates(model, text, max_candidates):\n",
    "    words = word_tokenize(text)\n",
    "    candidates = [None] * max_candidates\n",
    "    counter = 0\n",
    "    for word in words:\n",
    "        if wn.synsets(word) == []:\n",
    "            continue\n",
    "        tmp = wn.synsets(word)[0].pos()\n",
    "        # if not adjective or noun, continue\n",
    "        if tmp != \"a\" and tmp != \"n\":\n",
    "            continue\n",
    "        for a in antonyms(word):\n",
    "            candidates[counter] = (TreebankWordDetokenizer().detokenize([a if x == word else x for x in words]))\n",
    "            counter += 1\n",
    "            if counter >= max_candidates:\n",
    "                return list(filter(None.__ne__, candidates))\n",
    "    return list(filter(None.__ne__, candidates))\n",
    "\n",
    "def get_delta_opt(model, tokenizer, device, text, max_candidates):\n",
    "    cands = get_candidates(model, text, max_candidates)\n",
    "    max_prob = 0\n",
    "    found_cand = False\n",
    "    for c in cands:\n",
    "        input_ids, labels = get_tensors(device, tokenizer, c, 1.0)\n",
    "        cand_logits, cand_labels, cand_prob = get_pred(model, tokenizer, device, input_ids, labels)\n",
    "        if cand_prob > max_prob:\n",
    "            max_cand = c\n",
    "            max_prob = cand_prob\n",
    "            max_logits = cand_logits\n",
    "            max_prob = cand_prob\n",
    "            max_input_ids = input_ids\n",
    "            max_labels = labels\n",
    "            found_cand = True\n",
    "        else:\n",
    "            del cand_logits\n",
    "            del cand_labels\n",
    "            del cand_prob\n",
    "            del input_ids\n",
    "            del labels\n",
    "            torch.cuda.empty_cache()\n",
    "    if not found_cand:\n",
    "        input_ids, labels = get_tensors(device, tokenizer, text, 1.0)\n",
    "        max_logits, max_labels, max_prob = get_pred(model, tokenizer, device, input_ids, labels)\n",
    "        max_cand = text\n",
    "        del input_ids\n",
    "        del labels\n",
    "    return max_cand, max_logits, max_prob\n",
    "\n",
    "def get_pred(model, tokenizer, device, input_ids, labels):\n",
    "    outputs = model(input_ids)\n",
    "    logits = outputs[0]\n",
    "    pos_prob = torch.nn.Softmax(dim=-1)(logits)[:, -1]\n",
    "    return logits, labels, pos_prob\n",
    "\n",
    "def get_tensors(device, tokenizer, text, label):\n",
    "    encoding = tokenizer(text, return_tensors='pt', padding=True, truncation=True)['input_ids']\n",
    "    input_ids = encoding.to(device)\n",
    "    labels = torch.LongTensor([label]).to(device)\n",
    "    return input_ids, labels\n",
    "\n",
    "def train_nlp(model, tokenizer, weight_dir, thresholds_to_eval, recourse_loss_weight, max_candidates = 10):\n",
    "\n",
    "    training_file_name = weight_dir + str(recourse_loss_weight) + \"_model_training_info.txt\"\n",
    "\n",
    "    training_file = open(training_file_name, \"w\")\n",
    "\n",
    "    training_file = None\n",
    "    # get data\n",
    "    device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "    # load model and tokenizer    \n",
    "    train_texts, train_labels = get_sst_data('data/nlp_data/train.txt')\n",
    "    train_texts = train_texts\n",
    "    train_labels = train_labels\n",
    "\n",
    "\n",
    "    dev_texts, dev_labels = get_sst_data('data/nlp_data/dev.txt')\n",
    "    dev_texts = dev_texts\n",
    "    dev_labels = dev_labels\n",
    "\n",
    "    batch_size = 1\n",
    "\n",
    "    lr = 2e-5\n",
    "    num_warmup_steps = 0\n",
    "    num_epochs = 3\n",
    "    num_train_steps = len(train_texts)/batch_size * num_epochs\n",
    "\n",
    "\n",
    "    print(\"len(train): \", len(train_texts), file = training_file)\n",
    "    print(\"train # pos: \", np.sum(np.array(train_labels)), file = training_file)\n",
    "    print(\"train # neg: \", len(train_texts) - np.sum(np.array(train_labels)), file = training_file)\n",
    "    print(\"len(val): \", len(dev_texts), file = training_file)\n",
    "\n",
    "    print(\"\", file = training_file)\n",
    "\n",
    "    print(\"lr: \", lr, file = training_file)\n",
    "    print(\"num warmup steps: \", num_warmup_steps, file = training_file)\n",
    "    print(\"num epochs: \", num_epochs, file = training_file)\n",
    "    print(\"max candidates: \", max_candidates, file = training_file)\n",
    "\n",
    "    optim = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = get_linear_schedule_with_warmup(optim, num_warmup_steps, num_train_steps)\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight):\n",
    "        normal_loss = loss_fn(logits, labels)\n",
    "        recourse_loss = loss_fn(delta_logits, torch.LongTensor([1.0]).to(device))\n",
    "        return recourse_loss * recourse_loss_weight + normal_loss\n",
    "\n",
    "    best_val_loss = 100000000\n",
    "\n",
    "    flipped_by_thresh = {thresh: 0 for thresh in thresholds_to_eval}\n",
    "    negative_by_thresh = {thresh: 0 for thresh in thresholds_to_eval}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        batch_loss, train_epoch_loss, train_correct = 0, 0, 0\n",
    "        \n",
    "        print(\"EPOCH: \", epoch)\n",
    "        epoch_start = time.time()\n",
    "        model.train()\n",
    "        \n",
    "        pos_probs = []\n",
    "\n",
    "        for i, (text, label) in tqdm(enumerate(zip(train_texts, train_labels)), total = len(train_texts)):\n",
    "            input_ids, labels = get_tensors(device, tokenizer, text, label)\n",
    "            logits, labels, pos_prob = get_pred(model, tokenizer, device, input_ids, labels)\n",
    "            _, delta_logits, delta_prob = get_delta_opt(model, tokenizer, device, text, max_candidates)\n",
    "            batch_loss += combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight)\n",
    "                \n",
    "                \n",
    "            if i % batch_size == 0:    \n",
    "                model.zero_grad() \n",
    "                batch_loss.backward()\n",
    "                train_epoch_loss += batch_loss.item()\n",
    "                optim.step()\n",
    "                scheduler.step()\n",
    "                del batch_loss\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_loss = 0\n",
    "            \n",
    "            pos_probs.append(pos_prob.item())\n",
    "\n",
    "            for t in thresholds_to_eval:\n",
    "                if pos_prob.item() < t:\n",
    "                    negative_by_thresh[t] += 1\n",
    "                    if delta_prob.item() >= t:\n",
    "                        flipped_by_thresh[t] += 1\n",
    "            \n",
    "        np_probs = np.array(pos_probs)\n",
    "        np_labels = np.array(train_labels)\n",
    "\n",
    "        # write train info\n",
    "        print(\"----------\", file = training_file)\n",
    "        print(\"\", file = training_file)\n",
    "        print(\"EPOCH: \", epoch, file = training_file)\n",
    "        print(\"training time for epoch: \", round((time.time() - epoch_start)/60, 3), \" minutes\", file = training_file)\n",
    "        print(\"\", file = training_file)\n",
    "\n",
    "        for t_idx, t in enumerate(thresholds_to_eval):\n",
    "            label_preds = np.array([0.0 if a < t else 1.0 for a in np_probs])\n",
    "\n",
    "            f1 = round(f1_score(label_preds, np_labels), 3)\n",
    "\n",
    "            recall = round(recall_score(label_preds, np_labels), 3)\n",
    "\n",
    "            prec = round(precision_score(label_preds, np_labels), 3)\n",
    "\n",
    "            acc = round(np.sum(label_preds == np_labels)/np_labels.shape[0], 3)\n",
    "\n",
    "            num_neg = negative_by_thresh[t]\n",
    "            num_pos = len(train_labels) - num_neg\n",
    "            assert (num_neg + num_pos) == len(train_labels)\n",
    "            flipped = flipped_by_thresh[t]\n",
    "\n",
    "            if num_neg != 0:\n",
    "                flipped_proportion = round(flipped/num_neg, 3)\n",
    "            else:\n",
    "                flipped_proportion = 0\n",
    "\n",
    "            recourse_proportion = round((flipped + num_pos)/len(train_labels), 3)\n",
    "            \n",
    "            print(\"TRAIN STATS FOR THRESHOLD = \" + str(t) + \": \", file = training_file)\n",
    "            print(\"train acc: \", acc, file = training_file)\n",
    "            print(\"train f1: \", f1, file = training_file)\n",
    "            print(\"train flipped: \", flipped_proportion, file = training_file)\n",
    "            print(\"train recourse: \", recourse_proportion, file = training_file)\n",
    "            print(\"train pos preds: \", sum(label_preds)/len(label_preds), file = training_file)\n",
    "            print(\"train pos labels: \", sum(np_labels)/len(label_preds), file = training_file)            \n",
    "            print(\"\\n\")\n",
    "\n",
    "        print(\"Train (avg) epoch loss: \", train_epoch_loss/len(train_texts))\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "            \n",
    "        val_correct = 0\n",
    "        epoch_val_loss = 0\n",
    "        pos_probs = []\n",
    "\n",
    "        flipped_by_thresh = {thresh: 0 for thresh in thresholds_to_eval}\n",
    "        negative_by_thresh = {thresh :0 for thresh in thresholds_to_eval}\n",
    "\n",
    "        for i, (text, label) in tqdm(enumerate(zip(dev_texts, dev_labels)), total = len(dev_texts)):\n",
    "            input_ids, labels = get_tensors(device, tokenizer, text, label)\n",
    "            logits, labels, pos_prob = get_pred(model, tokenizer, device, input_ids, labels)            \n",
    "            _, delta_logits, delta_prob = get_delta_opt(model, tokenizer, device, text, max_candidates)\n",
    "            epoch_val_loss += combined_loss(model, device, logits, labels, delta_logits, loss_fn, recourse_loss_weight).item()\n",
    "            \n",
    "            del input_ids\n",
    "            del labels\n",
    "            del logits\n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            pos_probs.append(pos_prob.item())\n",
    "\n",
    "            for t in thresholds_to_eval:\n",
    "                if pos_prob.item() < t:\n",
    "                    negative_by_thresh[t] += 1\n",
    "                    if delta_prob.item() >= t:\n",
    "                        flipped_by_thresh[t] += 1\n",
    "                        \n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_model_name = weight_dir + str(recourse_loss_weight) + '_best_model.pt'\n",
    "            torch.save(model, best_model_name)\n",
    "            best_epoch = True\n",
    "\n",
    "        else:\n",
    "            best_epoch = False\n",
    "\n",
    "        # if best epoch, eval\n",
    "        np_probs = np.array(pos_probs)\n",
    "        np_labels = np.array(dev_labels)\n",
    "\n",
    "        if best_epoch:\n",
    "            best_model_info_file_name = weight_dir + str(recourse_loss_weight) + \"_best_model_val_info.txt\"\n",
    "            best_model_info_file = open(best_model_info_file_name, \"w\")\n",
    "            print(\"epoch: \", epoch, file = best_model_info_file)\n",
    "            best_model_info_file.close()\n",
    "\n",
    "        f1_by_thresh, recall_by_thresh, precision_by_thresh, acc_by_thresh, flipped_proportion_by_thresh, recourse_proportion_by_thresh = [], [], [], [], [], []\n",
    "\n",
    "        for t_idx, t in enumerate(thresholds_to_eval):\n",
    "            label_preds = np.array([0.0 if a < t else 1.0 for a in np_probs])\n",
    "\n",
    "            f1 = round(f1_score(label_preds, np_labels), 3)\n",
    "            f1_by_thresh.append(f1) \n",
    "\n",
    "            recall = round(recall_score(label_preds, np_labels), 3)\n",
    "            recall_by_thresh.append(recall)\n",
    "\n",
    "            prec = round(precision_score(label_preds, np_labels), 3)\n",
    "            precision_by_thresh.append(prec)\n",
    "\n",
    "            acc = round(np.sum(label_preds == np_labels)/np_labels.shape[0], 3)\n",
    "            acc_by_thresh.append(acc) \n",
    "\n",
    "            num_neg = negative_by_thresh[t]\n",
    "            num_pos = len(dev_labels) - num_neg\n",
    "            assert (num_neg + num_pos) == len(dev_labels)\n",
    "            flipped = flipped_by_thresh[t]\n",
    "\n",
    "            if num_neg != 0:\n",
    "                flipped_proportion = round(flipped/num_neg, 3)\n",
    "            else:\n",
    "                flipped_proportion = 0\n",
    "\n",
    "            recourse_proportion = round((flipped + num_pos)/len(dev_labels), 3)\n",
    "\n",
    "            flipped_proportion_by_thresh.append(flipped_proportion)\n",
    "            recourse_proportion_by_thresh.append(recourse_proportion)\n",
    "\n",
    "            print(\"VAL STATS FOR THRESHOLD = \" + str(t) + \": \", file = training_file)\n",
    "            print(\"val acc: \", acc, file = training_file)\n",
    "            print(\"val f1: \", f1, file = training_file)\n",
    "            print(\"val flipped: \", flipped_proportion, file = training_file)\n",
    "            print(\"val recourse: \", recourse_proportion, file = training_file)\n",
    "            print(\"val pos preds: \", sum(label_preds)/len(label_preds), file = training_file)\n",
    "            print(\"val pos labels: \", sum(np_labels)/len(label_preds), file = training_file)\n",
    "            print(\"\\n\")\n",
    "        \n",
    "        if best_epoch:\n",
    "\n",
    "            thresholds_data = {}\n",
    "\n",
    "            thresholds_data['thresholds'] = thresholds_to_eval\n",
    "            thresholds_data['f1s'] = f1_by_thresh\n",
    "            thresholds_data['accs'] = acc_by_thresh\n",
    "            thresholds_data['recalls'] = recall_by_thresh\n",
    "            thresholds_data['precisions'] = precision_by_thresh\n",
    "            thresholds_data['flipped_proportion'] = flipped_proportion_by_thresh\n",
    "            thresholds_data['recourse_proportion'] = recourse_proportion_by_thresh\n",
    "            thresholds_df = pd.DataFrame(data=thresholds_data)\n",
    "            \n",
    "            best_model_thresholds_file_name = weight_dir + str(recourse_loss_weight) + '_val_thresholds_info.csv'\n",
    "            thresholds_df.to_csv(best_model_thresholds_file_name, index_label='index')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/alexisross/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done.\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import os\n",
    "import torch\n",
    "from nlp_train_utils import *\n",
    "\n",
    "recourse_loss_weight = 0.0\n",
    "weight_dir = 'test_nlp/test/' + str(recourse_loss_weight) + \"/\"\n",
    "if not os.path.exists(weight_dir):\n",
    "    os.makedirs(weight_dir)\n",
    "    \n",
    "thresholds_to_eval = [0.5]\n",
    "device = torch.device('cuda:1') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "model, tokenizer = load_model(device, model_name = 'bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/6920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH:  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6920/6920 [1:12:31<00:00,  1.59it/s]\n",
      "  0%|          | 0/872 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Train (avg) epoch loss:  0.2570111471373652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 872/872 [08:34<00:00,  1.69it/s]\n",
      "  0%|          | 0/6920 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "EPOCH:  1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 436/6920 [04:29<1:22:43,  1.31it/s]"
     ]
    }
   ],
   "source": [
    "train_nlp(model, tokenizer, weight_dir, thresholds_to_eval, recourse_loss_weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
